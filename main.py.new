"""
Main script for the Sneaker Bot - MongoDB optimized version.
"""

import argparse
import colorama
import datetime
import logging
import random
import schedule
import time
import sys
from tqdm import tqdm

from config import (WEBSITES, MIN_DISCOUNT_PERCENT, BRANDS_OF_INTEREST,
                   SAVE_TO_CSV, CSV_FILENAME, DATABASE_ENABLED, DATABASE_PATH,
                   EMAIL_INTERVAL_MINUTES, EMAIL_NOTIFICATIONS,
                   SNEAKER_SITES, DEFAULT_SITES, MARKET_PRICE_SITES,
                   MONGODB_ENABLED, MONGODB_CONNECTION_STRING, MONGODB_DATABASE, MONGODB_COLLECTION)

from scrapers.factory import get_scraper_for_site
from notifications import EmailNotifier
from storage import DealStorage, MongoDBStorage
from utils.price_comparison import PriceComparer

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("SneakerBot")

# Initialize colorama for colored output
colorama.init()

# Global storage instances
mongodb_storage = None

def get_timestamp():
    """Get current timestamp for display."""
    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

def analyze_deals_for_profit(deals, price_comparer):
    """
    Analyze deals for profitability against market prices.
    
    Args:
        deals: List of deals to analyze
        price_comparer: PriceComparer instance with market data
        
    Returns:
        List of analyzed deals with profit calculations
    """
    if not deals:
        return []
    
    logger.info(f"Analyzing {len(deals)} deals for profitability...")
    
    analyzed_deals = []
    for deal in deals:
        # Calculate potential profit
        profit_analysis = price_comparer.calculate_profit_potential(deal)
        
        # Add profit analysis to deal
        deal.update(profit_analysis)
        
        # Mark as profitable if it meets criteria
        if deal.get('profit_potential', 0) > 0:
            deal['is_profitable'] = True
        
        analyzed_deals.append(deal)
    
    return analyzed_deals

def save_and_upload_deals(analyzed_deals, storage, args):
    """
    Save deals and upload to various storage systems.
    
    Args:
        analyzed_deals: List of analyzed deals
        storage: Local storage instance
        args: Command line arguments
        
    Returns:
        bool: True if we should continue iterating (for continuous mode)
    """
    global mongodb_storage
    
    # Get the most profitable deals
    price_comparer = PriceComparer()
    profitable_deals = price_comparer.get_most_profitable_deals(analyzed_deals)
    
    # Save deals if there are any
    if analyzed_deals:
        if SAVE_TO_CSV:
            storage.save_deals(analyzed_deals)
            
        if hasattr(args, 'export_json') and args.export_json:
            storage.export_to_json()
            
        # Upload to MongoDB (primary storage)
        if mongodb_storage is not None and MONGODB_ENABLED:
            mongodb_storage.upload_deals(analyzed_deals)
            
        # No cloud storage needed
        latest_file_key = None
            
        # Check if we should continue iterating
        should_continue = False
        if hasattr(args, 'iterate') and args.iterate and not hasattr(args, 'scheduled_run'):
            if mongodb_storage is not None and MONGODB_ENABLED:
                should_continue = mongodb_storage.continue_to_iterate([], analyzed_deals)
            else:
                # For local storage, use simple comparison
                should_continue = len(analyzed_deals) > 0
                
        return should_continue
    
    return False

def run_scraper(scraper, site_name):
    """
    Run a single scraper and return deals.
    
    Args:
        scraper: Scraper instance
        site_name: Name of the site being scraped
        
    Returns:
        List of deals found
    """
    deals = []
    
    try:
        logger.info(f"Searching {site_name} for real products with purchase links...")
        logger.info(f"Scraping {site_name}...")
        
        # Scrape the site
        site_deals = scraper.scrape()
        
        if site_deals:
            # Filter deals by minimum discount
            filtered_deals = [
                deal for deal in site_deals 
                if deal.get('discount_percent', 0) >= MIN_DISCOUNT_PERCENT
            ]
            
            # Filter by brand if configured
            if BRANDS_OF_INTEREST:
                filtered_deals = [
                    deal for deal in filtered_deals
                    if deal.get('brand', '').lower() in [b.lower() for b in BRANDS_OF_INTEREST]
                ]
            
            # Add a note that these are real products with working links
            for deal in filtered_deals:
                deal['is_real_product'] = True
                deal['verified_link'] = True
            
            logger.info(f"Found {len(filtered_deals)} deals on {site_name} meeting criteria")
            deals.extend(filtered_deals)
            
        logger.info(f"Found {len(deals)} real products on {site_name}")
    
    except Exception as e:
        logger.error(f"Error running scraper for {site_name}: {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
    
    return deals

def run_scraper_job(args, scheduled_run=False, last_iteration_deals=None):
    """
    Run the scraper job to find deals.
    
    Args:
        args: Command line arguments
        scheduled_run: Whether this is a scheduled run
        last_iteration_deals: Deals found in the previous iteration
    
    Returns:
        List of deals found
    """
    if scheduled_run:
        args.scheduled_run = True
    
    print(f"\n{colorama.Fore.YELLOW}Real Product Search - {get_timestamp()}{colorama.Style.RESET_ALL}")
    print("Searching for real products with working links...")
    
    # Initialize storage
    storage = DealStorage(
        csv_filename=CSV_FILENAME,
        db_enabled=DATABASE_ENABLED,
        db_path=DATABASE_PATH
    )
    
    # Initialize price comparer
    price_comparer = PriceComparer()
    
    # Always use real scraping mode
    args.test_mode = False
    
    # First, scrape market price sites to get comparison data
    logger.info("First collecting market price data for comparison...")
    for site_name in tqdm([s for s in args.sites if s in MARKET_PRICE_SITES], 
                         desc="Market Price Sites", 
                         unit="site"):
        try:
            scraper = get_scraper_for_site(site_name)
            if not scraper:
                logger.warning(f"No scraper available for {site_name}")
                continue
                
            logger.info(f"Scraping market prices from {site_name}...")
            prices = scraper.scrape_market_prices()
            
            if prices:
                # Add market prices to the price comparer
                for sku, price in prices.items():
                    price_comparer.add_market_price(sku, price, source=site_name)
                    
                logger.info(f"Found {len(prices)} market prices from {site_name}")
            else:
                logger.info(f"No market prices found from {site_name}")
                
        except Exception as e:
            logger.error(f"Error scraping market prices from {site_name}: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
    
    # Next, scrape retail sites for deals
    logger.info(f"\nNow scraping retail sites for deals...")
    
    # Set up scrapers for all sites
    scrapers = {}
    for site_name in args.sites:
        if site_name not in MARKET_PRICE_SITES:  # Skip market price sites
            scraper = get_scraper_for_site(site_name)
            if scraper:
                scrapers[site_name] = scraper
            else:
                logger.warning(f"No scraper available for {site_name}")
    
    # Run scrapers sequentially
    all_deals = []
    for site_name, scraper in tqdm(scrapers.items(), desc="Retail Sites", unit="site"):
        deals = run_scraper(scraper, site_name)
        all_deals.extend(deals)
    
    # Analyze deals for profitability
    logger.info(f"\nFound {len(all_deals)} total deals from all sites")
    analyzed_deals = analyze_deals_for_profit(all_deals, price_comparer)
    
    # Save and upload deals
    save_and_upload_deals(analyzed_deals, storage, args)
    
    # Print summary of findings
    print(f"\n{colorama.Fore.GREEN}Summary - {get_timestamp()}{colorama.Style.RESET_ALL}")
    print(f"Total deals found: {len(all_deals)}")
    
    profitable_deals = [d for d in analyzed_deals if d.get('is_profitable', False)]
    print(f"Profitable deals: {len(profitable_deals)}")
    
    # Sort by profit potential
    if profitable_deals:
        profitable_deals.sort(key=lambda x: x.get('profit_potential', 0), reverse=True)
        
        # Print top deals
        print(f"\n{colorama.Fore.CYAN}Top Profitable Deals:{colorama.Style.RESET_ALL}")
        max_to_show = min(5, len(profitable_deals))
        for i, deal in enumerate(profitable_deals[:max_to_show]):
            profit = deal.get('profit_potential', 0)
            profit_percent = deal.get('profit_percent', 0)
            print(f"{i+1}. {deal['title']} - ${deal['price']} (Market: ${deal.get('market_price', 'N/A')}) - Profit: ${profit:.2f} ({profit_percent:.1f}%)")
            print(f"   URL: {deal['url']}")
            print(f"   SKU: {deal.get('sku', 'N/A')}")
            print()
            
        # Show storage status
        print(f"\n{colorama.Fore.CYAN}Storage Status:{colorama.Style.RESET_ALL}")
        if SAVE_TO_CSV:
            print(f"  Results saved to: {CSV_FILENAME}")
        if DATABASE_ENABLED:
            print(f"  Database updated: {DATABASE_PATH}")
        if MONGODB_ENABLED and mongodb_storage is not None:
            print(f"  Results saved to MongoDB database: {MONGODB_DATABASE}")
    
    if not scheduled_run:
        print(f"\n{colorama.Fore.CYAN}Finished at: {get_timestamp()}{colorama.Style.RESET_ALL}")
    
    return analyzed_deals

def main():
    """Main function."""
    parser = argparse.ArgumentParser(description='Sneaker Deal Bot - MongoDB Optimized')
    
    # Site selection
    parser.add_argument('--sites', nargs='+', default=DEFAULT_SITES,
                        help='Sites to scrape (default: recommended sites)')
    parser.add_argument('--list-sites', action='store_true',
                        help='List available sites and exit')
    
    # Running mode
    parser.add_argument('--continuous', action='store_true',
                        help='Run continuously')
    parser.add_argument('--interval', type=int, default=60,
                        help='Interval between runs in seconds (default: 60)')
    parser.add_argument('--report-interval', type=int, default=1800,
                        help='Email report interval in seconds (default: 1800 - 30 minutes)')
    parser.add_argument('--iterate', action='store_true',
                        help='Keep scraping until no new deals are found')
    parser.add_argument('--max-iterations', type=int, default=5,
                        help='Maximum number of iterations when using --iterate')
    
    # Output options
    parser.add_argument('--verbose', action='store_true',
                        help='Enable verbose output')
    parser.add_argument('--export-json', action='store_true',
                        help='Export deals to JSON')
    parser.add_argument('--verify-sites', action='store_true',
                        help='Verify site availability before scraping')
    
    # Email options
    parser.add_argument('--email', action='store_true',
                        help='Enable email notifications (overrides config)')
    parser.add_argument('--no-email', action='store_true',
                        help='Disable email notifications')
    
    # Storage options
    parser.add_argument('--csv', action='store_true',
                        help='Enable CSV storage (overrides config)')
    parser.add_argument('--no-csv', action='store_true',
                        help='Disable CSV storage')
    
    # MongoDB options
    parser.add_argument('--mongodb', action='store_true',
                        help='Enable MongoDB storage (overrides config)')
    parser.add_argument('--no-mongodb', action='store_true',
                        help='Disable MongoDB storage')
    parser.add_argument('--mongodb-connection', help='MongoDB connection string')
    parser.add_argument('--mongodb-database', help='MongoDB database name')
    parser.add_argument('--mongodb-collection', help='MongoDB collection name')
    
    args = parser.parse_args()
    
    # Set verbose logging if requested
    if hasattr(args, 'verbose') and args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.setLevel(logging.DEBUG)
    
    # List sites if requested
    if hasattr(args, 'list_sites') and args.list_sites:
        print("\nAvailable sites:")
        print("-" * 20)
        
        print("\nRetail sites:")
        for site in sorted([s for s in SNEAKER_SITES if s not in MARKET_PRICE_SITES]):
            print(f" - {site}")
        
        print("\nMarket price sites:")
        for site in sorted(MARKET_PRICE_SITES):
            print(f" - {site}")
        
        print("\nDefault sites for scraping:")
        for site in DEFAULT_SITES:
            print(f" - {site}")
            
        sys.exit(0)
    
    # Initialize MongoDB storage if enabled
    global mongodb_storage
    mongodb_enabled = MONGODB_ENABLED
    if hasattr(args, 'mongodb') and args.mongodb:
        mongodb_enabled = True
    if hasattr(args, 'no_mongodb') and args.no_mongodb:
        mongodb_enabled = False
        
    if mongodb_enabled:
        # Setup MongoDB storage using command line args if provided, otherwise use config
        mongodb_storage = MongoDBStorage(
            connection_string=getattr(args, 'mongodb_connection', None) or MONGODB_CONNECTION_STRING,
            database_name=getattr(args, 'mongodb_database', None) or MONGODB_DATABASE,
            collection_name=getattr(args, 'mongodb_collection', None) or MONGODB_COLLECTION
        )
    
    # No cloud storage needed - using MongoDB only
    r2_enabled = False
    
    # Print configuration
    print(f"Scraping {len(args.sites)} sites: {', '.join(args.sites)}")
    print(f"Looking for brands: {', '.join(BRANDS_OF_INTEREST)}")
    print(f"Minimum discount: {MIN_DISCOUNT_PERCENT}%")
    print(f"Email notifications: {'Enabled' if EMAIL_NOTIFICATIONS else 'Disabled'}")
    print(f"MongoDB storage: {'Enabled' if mongodb_enabled else 'Disabled'}")
    print("-" * 80)
    
    # Run in continuous mode if specified
    if hasattr(args, 'continuous') and args.continuous:
        print(f"Running in continuous mode every {args.interval} seconds")
        print("-" * 80)
        
        # Run once immediately
        print("Running initial scan...")
        run_scraper_job(args)
        
        # Setup for continuous running
        start_time = datetime.datetime.now()
        
        # Schedule runs for email reports (every 30 minutes)
        def email_report_job():
            print(f"\n{colorama.Fore.GREEN}Sending email report - {get_timestamp()}{colorama.Style.RESET_ALL}")
            # The email will be sent automatically by the EmailNotifier class
            # Display uptime info
            uptime = datetime.datetime.now() - start_time
            uptime_str = str(uptime).split('.')[0]  # Remove microseconds
            next_email = datetime.datetime.now() + datetime.timedelta(seconds=args.report_interval)
            print(f"Uptime: {uptime_str} | Next email report: {next_email.strftime('%H:%M:%S')}")
        
        # Schedule runs for scraping (every 1-2 minutes, randomized to avoid detection)
        def scraper_job():
            print(f"\n{colorama.Fore.CYAN}Running scraper scan - {get_timestamp()}{colorama.Style.RESET_ALL}")
            run_scraper_job(args, scheduled_run=True)
            
            # Randomize next run time between specified interval and 1.5x that
            random_seconds = random.randint(args.interval, int(args.interval * 1.5))
            next_run = datetime.datetime.now() + datetime.timedelta(seconds=random_seconds)
            print(f"Next scan: {next_run.strftime('%H:%M:%S')}")
        
        # Schedule the jobs
        # Email reports every 30 minutes
        schedule.every(args.report_interval).seconds.do(email_report_job)
        
        # Scraper job runs at the specified interval with jitter
        schedule.every(args.interval).seconds.do(scraper_job)
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(1)  # Small sleep to prevent CPU thrashing
        except KeyboardInterrupt:
            print(f"\n{colorama.Fore.RED}Interrupted by user. Exiting...{colorama.Style.RESET_ALL}")
    
    # Iteration mode
    elif hasattr(args, 'iterate') and args.iterate:
        print(f"Running in iteration mode (max {args.max_iterations} iterations)")
        print("-" * 80)
        
        last_deals = []
        iteration = 1
        
        while iteration <= args.max_iterations:
            print(f"\n{colorama.Fore.CYAN}Iteration {iteration}/{args.max_iterations} - {get_timestamp()}{colorama.Style.RESET_ALL}")
            
            # Run the scraper job
            current_deals = run_scraper_job(args, last_iteration_deals=last_deals)
            
            # Check if we should continue
            should_continue = False
            if mongodb_storage is not None and MONGODB_ENABLED:
                should_continue = mongodb_storage.continue_to_iterate(last_deals, current_deals)
            else:
                should_continue = len(current_deals) > 0 and current_deals != last_deals
            
            if not should_continue:
                print(f"\n{colorama.Fore.GREEN}No new deals found. Stopping iteration.{colorama.Style.RESET_ALL}")
                break
            
            last_deals = current_deals
            iteration += 1
            
            print(f"Waiting 2 minutes before next iteration...")
            time.sleep(120)  # Wait 2 minutes between iterations
    
    # Single run mode
    else:
        run_scraper_job(args)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n{colorama.Fore.RED}Interrupted by user. Exiting...{colorama.Style.RESET_ALL}")
        sys.exit(0)
